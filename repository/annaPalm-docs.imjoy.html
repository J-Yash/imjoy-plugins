<docs lang="markdown">
This plugin renders the documentation for the ANNA-PALM plugin.
</docs>

<config lang="json">
{
  "name": "ANNA-PALM-docs",
  "type": "window",
  "tags": [],
  "ui": "",
  "version": "0.1.2",
  "cover": "",
  "description": "Documentation for ANNA-PALM plugin.",
  "icon": "extension",
  "inputs": null,
  "outputs": null,
  "api_version": "0.1.5",
  "env": "",
  "requirements": [
        "https://cdnjs.cloudflare.com/ajax/libs/vue/2.6.10/vue.min.js",
        "https://cdnjs.cloudflare.com/ajax/libs/marked/0.6.2/marked.js",
        "https://static.imjoy.io/spectre.css/spectre.min.css",
        "https://static.imjoy.io/spectre.css/spectre-exp.min.css",
        "https://static.imjoy.io/spectre.css/spectre-icons.min.css"],
  "dependencies": [],
  "defaults": {"w": 30, "h": 20}
}
</config>


<attachment name="summary">
<br>
<p id="summary">
  This plugin implements ANNA-PALM, which uses deep learning to reconstruct super-resolution
  views from sparse, rapidly acquired localization images and/or widefield images.
 </p>

More details about ANNA-PALM can be found here:

* **Publication**: Ouyang et. al, <a onclick="api.utils.openUrl('https://www.nature.com/articles/nbt.4106')">Deep learning massively accelerates super-resolution localization microscopy</a>,
Nature Biotechnology, 2018
* **GitHub**: <a onclick="api.utils.openUrl('https://github.com/imodpasteur/ANNA-PALM')">https://github.com/imodpasteur/ANNA-PALM</a>

Here, we provide workflows to train and use ANNA-PALM to reconstruct super-resolution views from sparse localization and widefield images.
We further provide example data for PALM experiments performed on microtubules.

<figure>
    <img src="https://www.dropbox.com/s/4a0f9h8i0svf3h2/anna-palm-example.png?dl=1" width="700" class="img-responsive p-centered" alt="anna-palm-reconstruction">
    <figcaption>Example of ANNA-PALM reconstruction: (left) input = sparse localization image, (right) output = predicted super-resolution image.</figcaption>
</figure>

## Installing the plugin and main features

If you don't already have the plugin, you can install it from this <a onclick="api.utils.openUrl('https://imjoy.io/#/app?w=care&plugin=oeway/ImJoy-Plugins:ANNA-PALM@GPU')">**link.**</a>

When starting the ANNA-PALM plugin, you will see a **central launch pad** allowing to select the task to be performed:

<figure>
    <img src="https://www.dropbox.com/s/ahpjey8fs7flz7q/anna-palm-launchpad.png?dl=1" class="img-responsive p-centered" alt="anna-palm-launchpad">
    <figcaption>ANNA-PALM plugin launchpad.</figcaption>
</figure>

The **main features** are:
* Training can be performed either on a provided remote plugin engine or a local plugin engine.
* Trained model can be loaded and used for prediction or re-training of both simulations and experimental data.
* Prediction can be performed once the model is trained.

</attachment>

<attachment name="load-model">
<br>
<p style="color:#FF0000"> FOR IMJOY REVIEWERS: if you use the provided remote-engine, a pre-trained model is available at`imjoy/imjoy-paper/ANNA-PALM-MODEL`.</p>
<br>
<p>
Here we describe how to load a pre-trained model that can be used either directly for prediction or for re-training.
</p>

## Trained model for microtubulins
A pre-trained model for microtubulins is available <a onclick="api.utils.openUrl('https://www.dropbox.com/sh/0ze0iq1pkxntszc/AADpXGwkRu894f4idjoNCEpSa?dl=0')">**here**.</a>
1. The actual model is contained in the folder `__model__`.
2. After downloading, you can load the trained model to the plugin engine (See FAQ for how to load data on the engine).

## Load model into ANNA-PALM plugin
Pressing the button `Load trained model` will open a file-dialog where you can select
a model that is stored on the plugin engine on which the plugin is running.

This model is then available either for re-training or prediction.

</attachment>


<attachment name="training">
<br>
<p style="color:#FF0000"> FOR IMJOY REVIEWERS: for immediating testing, you can use the provided remote engine 'imjoy.pasteur.cloud' with the 'GPU' tag.
  Further, training data for microtubules is already available at 'imjoy/imjoy-paper/ANNA-PALM-DATA'.
</p>
<br>
<p id="training">
Here we describe how to select a plugin engine, a computational environment, and the training data to perform training.
</p>

## Training data
* Training data consist of widefield and dense PALM/STORM images. The localization data must be
provided using the csv format of ThunderSTORM. These data pairs have the same filename, except the extension.
Training data will then be performed on sub-sampled PALM/STORM images.
* Training and validation data are in separate folders 'train' and 'test'.

Training data (the folder containing subfolder 'train' and 'test') has to be loaded onto
the plugin engine, where the ANNA-PALM plugin is running. See FAQ for how to load data
on the engine.

Experimental test data is available <a onclick="api.utils.openUrl('https://www.dropbox.com/sh/x6iy0gb928lk956/AADPLbIgsLaF8QcvnmX7qt72a?dl=0')">**here**.</a>

The provided example data for the microtubules looks like this:

```
├─ experimental_microtubules/
│  ├─ test/
│  │  ├─ Ctrl-lowDens_pos2.csv
│  │  ├─ Ctrl-lowDens_pos2.png
│  ├─ train/
│  │  ├─ Ctrl-lowDens_pos08.csv
│  │  ├─ Ctrl-lowDens_pos08.png
│  │  ├─ Ctrl-lowDens_pos10.csv
│  │  ├─ Ctrl-lowDens_pos10.png
│  │  ├─ ...
```

## Where to perform training: plugin engine
Training is performed on an ImJoy plugin engine, which can either run remotely or on your
own local workstation. To install a local plugin engine, please follow
<a onclick="api.utils.openUrl('https://github.com/oeway/ImJoy-App/releases')">**these instructions.**</a>
Once your ImJoy app is connected to one (or more) plugin engine(s), you can choose on which engine the CARE
plugin should run. For this press on the icon next to the plugin name in the plugin menu.

<figure>
    <img src="https://www.dropbox.com/s/mad93spddwgwpif/anna-palm-plugin-entry.png?dl=1" width="350" class="img-responsive p-centered" alt="screen-shot-care">
    <figcaption>ANNA-PALM plugin in the plugin menu.</figcaption>
</figure>
This will show a dropdown menu, where you can determine how and where the plugin is running.
In the lower part of the dropdown menu you can then choose on which of the the available plugin engines the plugin should run.
<figure>
    <img src="https://www.dropbox.com/s/6pb5nzflveo71h5/anna-palm-plugin-engines.png?dl=1" width="200" class="img-responsive p-centered" alt="plugin-engines">
    <figcaption>Choosing the plugin engine.</figcaption>
</figure>

## GPU or CPU computation
Training can be performed on CPUs or GPUs. The latter is substantially faster. To switch
between these computational modes, you can select the corresponding "tag". Currently supported are
<figure>
    <img src="https://www.dropbox.com/s/ibbeoefj8pcixv1/anna-palm-plugin-tags.png?dl=1" width="150" class="img-responsive p-centered" alt="tags">
    <figcaption>Choosing between GPU and CPU computing.</figcaption>
</figure>

## Monitoring training progress
Once training data is provided, the training a dashboard will be shown that allows to inspect training progress.
* In the upper part the dashboard shows the loss functions for the training and validation data. You can hoover
over the curves to see the current value and what loss function it is. The slider allows you to scroll through the different training steps and inspect how the quality
 of the prediction changes.

<figure>
    <img src="https://www.dropbox.com/s/2zn1t6mnognavst/anna-palm-loss.png?dl=1" width="500" class="img-responsive  p-centered" alt="training-loss">
    <figcaption>Transferring training data to the plugin engine.</figcaption>
</figure>

* In the lower part it shows the prediction results on a randomly selected image.
 'A' stands for sparse (input image), 'B' for high-resolution image. The following images are shown
    - real_A: sparse input image
    - real_B: high-resolution target image
    - reco_B: reconstructed high-resolution image
    - squirrel_error_map: error map calculate with the squirrel approach

If you want to **terminate the training** click on the icon next to the plugin name,
select `Terminate` and then close the window with the red x.

## Trained model
Once training is finished, the trained model can be used for prediction.

</attachment>

<attachment name="prediction">
To predict an image, you can either load a pre-trained model into the engine, or
train a new model. For either option, we refer to the dedicated sections in this documentation.


1. Press on `Predict`. This will then use either the loaded model, or (if available) the newly trained one.
2. Specify the folder the data you want to predict (it must contain a 'test' subfolder), e.g. the folder of the provided example data
3. This will then open a new interface, where progress on prediction is reported.
   The plugin will use the data in the 'test' folder.
   It will create 30 different samples, by sub-sampling with vastly different framenumbers
5. Once prediction is finished, you can scroll throught these data. 'Step' corresponds to one
 created data-set.

<figure>
    <img src="https://www.dropbox.com/s/1hs0jemw183jg9r/anna-palm-prediction.png?dl=1" class="img-responsive p-centered" alt="screen-shot-care">
    <figcaption>ANNA-PALM prediction interface.</figcaption>
</figure>

</attachment>

<attachment name="faq">
Here we provide answer to frequenty asked questions and encountered problems.

## How can I load data or a model into the plugin engine?
You can simply drag and drop a folder containing either training data or model:

1. In ImJOy, press `Files`, and `Open Engine File`.
2. If ImJoy is connected to multiple plugin engines, select the one where ANNA-PALM plugin is running in the upper part of the interface.
3. Navigate to the folder where you want to store your data.
4. Drag & drop the folder from Finder (MacOS) or Explorer (WIN) into the ImJoy file dialog.

<figure>
    <img src="https://www.dropbox.com/s/6dih56s3buf4y48/anna-palm-load-model-to-engine.gif?dl=1" width="700" class="img-responsive p-centered" alt="load-model-into-engine">
    <figcaption>Load a pre-trained model on a plugin engine.</figcaption>
</figure>

</attachment>


<script lang="javascript">
class ImJoyPlugin {
  async setup() {
    // For vue
    this.app = new Vue({
      el: '#app',
      data: {
        active_tab: 'summary'
      },
      methods: {
        activateTab(name){
          this.active_tab = name
        },
        focus(tab_name, section_id){
          if(tab_name && section_id){
            this.active_tab = tab_name
            this.nextTick(()=>{
              document.getElementById(section_id).scrollIntoView()
            })
          }
        }
      }
    })
    api.log('initialized')
  }
  async run(ctx) {

      document.getElementById('cont_summary').innerHTML =
      marked(await api.getAttachment('summary'));
      document.getElementById('cont_loadModel').innerHTML =
      marked(await api.getAttachment('load-model'));
      document.getElementById('cont_training').innerHTML =
      marked(await api.getAttachment('training'));
      document.getElementById('cont_prediction').innerHTML =
      marked(await api.getAttachment('prediction'));
      document.getElementById('cont_faq').innerHTML =
      marked(await api.getAttachment('faq'));
      try {
        console.log(ctx.data.tab, ctx.data.section)
        this.app.focus(ctx.data.tab, ctx.data.section)
        this.app.$forceUpdate()
      }
      catch(err) {
      console.log(err)
      }
  }
}
api.export(new ImJoyPlugin())
</script>

<window lang="html">
  <div id="app">

    <ul class="tab tab-block">
      <li class="tab-item" v-bind:class="{ active: active_tab=='summary' }">
        <a href="#" @click="active_tab='summary'">Summary</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='load-model' }">
        <a href="#" @click="active_tab='load-model'">Load model</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='training' }">
        <a href="#" @click="active_tab='training'">Training</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='prediction' }">
        <a href="#" @click="active_tab='prediction'">Prediction</a>
      </li>
      <li class="tab-item" v-bind:class="{ active: active_tab=='faq' }">
        <a href="#" @click="active_tab='faq'">FAQ</a>
      </li>
    </ul>

   <!-- This can eventually be replaced by a better organization, e.g. tabs -->
   <div v-show="active_tab=='summary'" class="tab-content" id="cont_summary"></div>
   <div v-show="active_tab=='load-model'" class="tab-content" id="cont_loadModel"></div>
   <div v-show="active_tab=='training'" class="tab-content" id="cont_training"></div>
   <div v-show="active_tab=='prediction'" class="tab-content" id="cont_prediction"></div>
   <div v-show="active_tab=='faq'" class="tab-content" id="cont_faq"></div>
  </div>
</window>

<style lang="css">

.tab-content{
  padding: 10px;
}

.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 90%;
}

figcaption {
    background-color: #fff;
    color: #a1c8f2;
    font: italic smaller sans-serif;
    padding: 3px;
    text-align: center;
}
</style>
