<docs lang="markdown">
#CARE

Weigert et. al, Content-aware image restoration: pushing the limits of fluorescence microscopy, Nature Methods, 2018

[Paper on Nature Methods](https://www.nature.com/articles/s41592-018-0216-7)

This is a demo plugin ported from https://github.com/CSBDeep/CSBDeep/blob/master/examples/denoising2D_probabilistic/1_training.ipynb

</docs>

<config lang="json">
{
  "name": "CARE",
  "type": "native-python",
  "version": "0.1.8",
  "api_version": "0.1.2",
  "description": "This plugin demonstrate denoising using CARE",
  "tags": ["CPU", "GPU", "macOS CPU"],
  "ui": "",
  "inputs": null,
  "outputs": null,
  "flags": [],
  "icon": "extension",
  "env": {
      "CPU": "conda create -n care-cpu python=3.6.7",
      "GPU":  "conda create -n care-gpu python=3.6.7",
      "macOS CPU": "conda create -n care-mac-cpu python=3.6.7"
  },
  "requirements": {
      "CPU": ["pip: tensorflow==1.8.0 Pillow csbdeep tifffile six"],
      "GPU": ["pip: tensorflow-gpu==1.8.0 Pillow csbdeep tifffile six gputil"],
      "macOS CPU": ["pip: tensorflow==1.5.0 Pillow csbdeep tifffile six"]
  },
  "dependencies": ["oeway/ImJoy-Plugins:Im2Im-Dashboard", "oeway/ImJoy-Plugins:CARE-docs","https://gist.githubusercontent.com/oeway/961c8d7abe24383d3ad6312669fe1d7c/raw/launchpad.imjoy.html"],
  "cover": ["https://dl.dropbox.com/s/bo8timmql033xy1/CARE-0.1.8.gif", "https://dl.dropbox.com/s/rq5ikbben3xbxge/CARE-0.1.8-2.gif"]
 }
</config>

<script lang="python">
from __future__ import print_function, unicode_literals, absolute_import, division
import os
import asyncio
import random
import numpy as np
from tifffile import imread

if api.TAG == 'GPU':
    import GPUtil
    # Set CUDA_DEVICE_ORDER so the IDs assigned by CUDA match those from nvidia-smi
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"

    # Get the first available GPU
    DEVICE_ID_LIST = GPUtil.getFirstAvailable()
    api.log(f'Available GPUs: {DEVICE_ID_LIST}')
    if len(DEVICE_ID_LIST)<= 0:
        api.alert('No GPU available')
        raise Exception('No GPU available')

    DEVICE_ID = DEVICE_ID_LIST[0] # grab first element from list
    api.log(f'Set GPU id to : {DEVICE_ID}')
    # Set CUDA_VISIBLE_DEVICES to mask out all other GPUs than the first available device id
    os.environ["CUDA_VISIBLE_DEVICES"] = str(DEVICE_ID)

import concurrent.futures
from imjoy import api

import tensorflow as tf
import base64
from io import BytesIO
from PIL import Image

from keras.callbacks import Callback
from csbdeep.utils.plot_utils import to_color
from csbdeep.utils import download_and_extract_zip_file, axes_dict, plot_some, plot_history
from csbdeep.utils.tf import limit_gpu_memory
from csbdeep.io import load_training_data
from csbdeep.models import Config, CARE
from csbdeep.data import RawData, create_patches
import json


# api.log(str(os.environ))

if os.path.exists('/imjoy/imjoy-paper'):
    ROOT_DIR = '/imjoy/imjoy-paper'
else:
    ROOT_DIR = ''

if tf.test.gpu_device_name():
    api.log('Default GPU Devices: {}'.format(tf.test.gpu_device_name()))
else:
    api.log("No GPU device is avilable")

def plot_tensors(dash, tensor_list, label, titles):
    # randomly select a Z
    index = np.random.randint(0, tensor_list[0].shape[0])
    image_list = [tensor[index, :,:] for tensor in tensor_list]
    displays = {}
    titles = titles or [ 'Tensor '+str(i) for i in range(len(image_list))]
    for i in range(len(image_list)):
        im = image_list[i]

        im = im[ :,:]
        min = 0 # im.min()
        normalized_im = np.clip(((im-min)/(im.max()-min)*255), 0, 255)
        im = Image.fromarray(normalized_im.astype('uint8'))

        buffered = BytesIO()
        im.save(buffered, format="JPEG")
        img_str = base64.b64encode(buffered.getvalue()).decode('ascii')
        imgurl = 'data:image/png;base64,' + img_str
        displays[titles[i]] = imgurl
    dash.appendDisplay(label, displays)


async def plot_all_tensors(dash, result_dir, result_url, tensor_list, label, titles):
    # randomly select a Z

    for slice in range(tensor_list[0].shape[0]):
        image_list = [tensor[slice, :,:] for tensor in tensor_list]
        displays = {}
        titles = titles or [ 'Tensor '+str(i) for i in range(len(image_list))]
        for i in range(len(image_list)):
            im = image_list[i]
            api.log(str(im.shape))

            im = im[ :,:]
            min = 0 # im.min()
            normalized_im = np.clip(((im-min)/(im.max()-min)*255), 0, 255)
            im = Image.fromarray(normalized_im.astype('uint8'))

            sample_dir = os.path.join(result_dir, label)
            if not os.path.exists(sample_dir):
                os.makedirs(sample_dir)

            im.save(os.path.join(sample_dir, label+'_'+titles[i]+'_slice_'+str(slice) + '.png'), format="png")
            #displays[titles[i]] = result_url + '/' + label + '/' + label+titles[i]+'_slice_'+str(slice) + '.png'
            buffered = BytesIO()
            im.save(buffered, format="JPEG")
            img_str = base64.b64encode(buffered.getvalue()).decode('ascii')
            imgurl = 'data:image/png;base64,' + img_str
            displays[titles[i]] = imgurl
        await dash.appendDisplay(label + '_slice ' + str(slice), displays)

class UpdateUI(Callback):
    def __init__(self, model, total_epoch, dash, config, val_data=None):
        self.total_epoch = total_epoch
        self.epoch = 0
        self.logs = {}
        self.dash = dash
        self.step = 0
        self.config = config
        self.m = model
        if val_data is not None:
            self.val_x, self.val_y, self.axes = val_data
        else:
            self.val_x, self.val_y, self.axes = None, None, None

    def on_batch_end(self, batch, logs):
        if batch % 10 == 0:
            self.logs = logs
            api.showStatus('training epoch:'+str(self.epoch)+'/'+str(self.total_epoch))
            api.log('batch:'+str(batch) + ' '+ str(logs))
            self.dash.updateCallback('onStep', self.step, {'loss': np.asscalar(logs['loss']), 'mse': np.asscalar(logs['mse'])})
        self.step += 1
    def on_epoch_end(self, epoch, logs):
        self.epoch = epoch
        self.logs = logs
        self.dash.updateCallback('onStep', self.step, {'val_loss': np.asscalar(logs['val_loss'])})
        api.showProgress(self.epoch/self.total_epoch*100)
        api.showStatus('training epoch:'+str(self.epoch)+'/'+str(self.total_epoch))
        api.log('epoch:'+str(self.epoch)+'/'+str(self.total_epoch) + ' '+ str(logs))
        x, y, axes = self.val_x, self.val_y, self.axes
        if x is not None:
            try:
                restored = self.m.predict(x, axes)
            except Exception as e:
                api.error(str(e))
                restored = self.m.predict(x, axes, n_tiles=(1,4,4))
            label = 'Step '+ str(self.step)
            if y is not None:
                tensor_list = [x, restored, y]
                titles = ['input', "output", 'target']
            else:
                tensor_list = [x, restored]
                titles = ['input', "output"]
            plot_tensors(self.dash, tensor_list, label, titles)

loop = asyncio.get_event_loop()
class ImJoyPlugin():
    def setup(self):
        print('setup in python')

    def train(self, data_file, epochs, model_name, data_config, val_files):
        asyncio.set_event_loop(loop)
        self.dash.setLoading({'status_text': 'Loading data...', 'loading': True})
        (X,Y), (X_val,Y_val), axes = load_training_data(data_file, validation_split=0.1, verbose=True)
        api.log(str(X.shape))
        c = axes_dict(axes)['C']
        n_channel_in, n_channel_out = X.shape[c], Y.shape[c]

        config = Config(axes, n_channel_in, n_channel_out, probabilistic=True, train_epochs=epochs, train_steps_per_epoch=20)
        print(config)

        api.showStatus('creating model ...')
        self.dash.setLoading({'status_text': 'Creating model...', 'loading': True})
        basedir = 'models'
        model = CARE(config, model_name, basedir=basedir)
        care_data_config = os.path.join(basedir, model_name, "care_data_config.json")
        with open(care_data_config, "w") as write_file:
            json.dump(data_config, write_file)

        val_x_file, val_y_file, val_axes = val_files
        if val_x_file is not None and os.path.exists(val_x_file):
            val_x = imread(val_x_file)
        else:
            val_x = None
        if val_y_file is not None and os.path.exists(val_y_file):
            val_y = imread(val_y_file)
        else:
            val_y = None
        axes = val_axes

        updateUI = UpdateUI(model, config.train_steps_per_epoch, self.dash, config, (val_x, val_y, axes))

        api.showStatus('start training')
        self.dash.setLoading({'status_text': 'Start training...', 'loading': False})
        model.prepare_for_training()
        model.callbacks.append(updateUI)
        history = model.train(X,Y, validation_data=(X_val,Y_val))
        print(sorted(list(history.history.keys())))
        api.showStatus('training stoped')

    async def prepare_data(self, data_file, source_dirs, target_dir):
        if data_file.startswith('http') and data_file.endswith('.zip'):
            filename = data_file.split('/')[-1]
            name, _ = os.path.splitext(filename)

            skip_download = False
            if not os.path.exists(os.path.join('data', name, 'train')):
                ret = await api.confirm(content='It seem the file you try to download already exists, do you want to downlaoad and process again?', confirm_text='Yes, download', cancel_text= 'No')
                if not ret:
                    skip_download =True

            if not skip_download:
                await api.showStatus('Downloading data ' + filename + '...')
                download_and_extract_zip_file (
                    url       = data_file,
                    targetdir = 'data',
                )
                await api.showStatus('Downloaded ' + filename )

            if not os.path.isdir(os.path.join('data', name, 'train')):
                api.error('Failed to detect train folder.')
                await api.alert('You zip file must contain a folder called `train`')
                return

            data_file = os.path.join('data', name)
        elif data_file.startswith('http'):
            api.alert('only zip file can be downloaded and used for now.')
            return
        elif data_file.startswith('.npz'):
            if os.path.exists(data_file):
                return data_file, ( None, None, 'ZYX')
            else:
                await api.alert(data_file + " doesn't exist.")
                return

        # handle data folder
        data_dir = data_file
        _, name = os.path.split(data_dir)
        await api.showStatus('Loading data from ' + data_dir + '/train' )
        raw_data = RawData.from_folder (
            basepath    = os.path.join(data_dir, 'train'),
            source_dirs = source_dirs,
            target_dir  = target_dir,
            axes        = 'ZYX',
        )

        npz_path = data_dir+ '.npz'
        await api.showStatus('Creating patches for ' + data_dir + '/train' )
        X, Y, XY_axes = create_patches (
            raw_data            = raw_data,
            patch_size          = (16,64,64),
            n_patches_per_image = 1024,
            save_file           = npz_path,
        )
        await api.showStatus('Patches generated: '+str(X.shape))
        api.log('Patches generated: '+str(X.shape))

        await api.showStatus('Reading test files from ' + data_dir + '/test' )
        source_file = None
        target_file = None
        if os.path.isdir(os.path.join(data_dir, 'test')):
            source_path = os.path.join(data_dir, 'test', source_dirs[0])
            target_path = os.path.join(data_dir, 'test', target_dir)
            if os.path.exists(source_path):
                source_files = os.listdir(source_path)
                if len(source_files) > 0:
                    source_file = os.path.join(source_path, source_files[0])
                    target_file = os.path.join(target_path, source_files[0])
                    if not os.path.exists(target_file):
                        target_file = None

        val_files = ( source_file, target_file, 'ZYX')
        api.log(f'validation files: {val_files}')
        await api.showStatus('Data prepared.')
        return npz_path, val_files

    async def start_train(self, data_file):
        ret = await api.showDialog( name="Training Configurations", ui= "<br>".join([
            "CARE Model Name {id: 'model_name', type: 'string', placeholder: 'my_model'}",
            "Epochs { id: 'epochs', type:'number', placeholder: 30}",
            "Source folder name { id: 'source_dir', type:'string', placeholder: 'low'}",
            "Target folder name { id: 'target_dir', type:'string', placeholder: 'GT'}",
            ]))

        model_name = ret.model_name
        epochs = int(ret.epochs)
        source_dir = ret.source_dir
        target_dir = ret.target_dir
        source_dirs = [s.strip() for s in source_dir.split(',') if s.strip() != '']

        data_config = {}
        data_config['source_dirs'] = source_dirs
        data_config['target_dir'] = target_dir
        self.dash = await api.createWindow(type="Im2Im-Dashboard", name="CARE Training", w=25, h=20, data={"display_mode": "all", 'metrics': ['loss', 'val_loss', 'mse'], 'callbacks': ['onStep']})
        await self.dash.setLoading({'status_text': 'Preparing data...', 'loading': True})
        try:
            ret = await self.prepare_data(data_file, source_dirs, target_dir)
            if not ret:
                api.error('failed to prepare data.')
                await self.dash.setLoading({'status_text': 'Failed to prepare data', 'loading': False})
                return
        except:
            await self.dash.setLoading({'status_text': 'Failed to prepare data', 'loading': False})
            return
        npz_file, val_files = ret
        await self.dash.setLoading({'status_text': 'Start training', 'loading': True})
        await loop.run_in_executor(None, self.train, npz_file, epochs, model_name, data_config, val_files)

    async def train_with_url(self):
        self.dialog.close()
        url = await api.prompt('Please paste your file url here: ', 'http://csbdeep.bioimagecomputing.com/example_data/tribolium.zip')
        await self.start_train(url)

    async def train_with_folder(self):
        self.dialog.close()
        data_dir = await api.showFileDialog(title="Please select a folder with trianing data (subfolder: train and test)", root=os.path.join(ROOT_DIR, "CARE", "tribolium"), type= 'directory', engine= api.ENGINE_URL)
        await self.start_train(data_dir)

    async def train_with_npz(self):
        self.dialog.close()
        data_file = await api.showFileDialog(type= 'file', engine= api.ENGINE_URL)
        await self.start_train(data_file)

    async def predict_folder(self):
        self.dialog.close()
        model_dir, care_data_config = await self.select_model()
        basedir, name = os.path.split(model_dir)
        model = CARE(config=None, name=name, basedir=basedir)

        # await api.alert('Please select a test folder contains source folder named ' + ','.join(care_data_config['source_dirs']))
        source_dir_names = ','.join(care_data_config['source_dirs'])
        test_data_dir = await api.showFileDialog(title=f"Select a folder with test data (contains {source_dir_names})", root=os.path.join(ROOT_DIR, "CARE", "tribolium"), type="directory", engine=api.ENGINE_URL)
        if os.path.exists(os.path.join(test_data_dir, 'test')):
            test_data_dir = os.path.join(test_data_dir, 'test')
        if not all([os.path.exists(os.path.join(test_data_dir, source_dir)) for source_dir in care_data_config['source_dirs']]):
            await api.alert('You folder does not seem to have the correct format')
            return
        else:
            # TODO: only support one input source at the moment
            source_dir = os.path.join(test_data_dir, care_data_config['source_dirs'][0])
            target_dir = os.path.join(test_data_dir, care_data_config['target_dir'])
            source_images = [os.path.join(source_dir, f) for f in os.listdir(source_dir)]
            target_images = [os.path.join(target_dir, f) for f in os.listdir(source_dir)]

            api.log(f'source_images: {source_images}; target_images: {target_images}')
            result_dir = os.path.join(test_data_dir, 'CARE_results')
            if not os.path.exists(result_dir):
                os.makedirs(result_dir)
            result_url = await api.getFileUrl(path=result_dir, engine=api.ENGINE_URL)

            self.dash = await api.createWindow(type="Im2Im-Dashboard", name="CARE Prediction", w=25, h=20, data={"display_mode": "all"})
            await self.dash.setLoading({'status_text': 'Running prediction...', 'loading': True})
            axes = 'ZYX'
            sample_count = 0
            for x_file, y_file in zip(source_images, target_images):
                x = imread(x_file)
                y = imread(y_file) if os.path.exists(os.path.join(target_dir, y_file)) else None
                restored = model.predict(x, axes)
                api.log(str(restored.shape) + str(x.shape))
                _, file_name = os.path.split(x_file)
                label = str(sample_count) + '_' + file_name
                if y is not None:
                    tensor_list = [x, restored, y ]
                    titles = ['input', "output", 'target']
                else:
                    tensor_list = [x, restored ]
                    titles = ['input', "output"]
                await plot_all_tensors(self.dash, result_dir, result_url, tensor_list, label, titles)
                sample_count += 1
            await self.dash.setLoading({'status_text': f'Prediction finished, {sample_count} sample processed', 'loading': False})
        api.log('image size ='+ str(x.shape))
        api.alert('Prediction done.')

    async def select_model(self):
        self.dialog.close()
        model_dir = await api.showFileDialog(title="Please select a folder with trained CARE model", root=os.path.join(ROOT_DIR, "CARE", "models"), type="directory", engine=api.ENGINE_URL)
        if not os.path.exists(os.path.join(model_dir, 'config.json')) or not os.path.exists(os.path.join(model_dir, "care_data_config.json")):
            await api.alert('This folder does not seem to contain a model, please select another one.')
            return

        with open(os.path.join(model_dir, "care_data_config.json"), "r") as read_file:
            care_data_config = json.load(read_file)

        return model_dir, care_data_config

    async def show_docs(self):
        self.dialog.close()
        try:
            await this.win_docs.run({'data': {}})

        except:
            this.win_docs = await api.createWindow({
                'name': 'Documentation - CARE',
                'type': 'CARE-docs',
                'w':30, 'h':20,
                'data': {}
                })


    async def export_model(self):
        self.dialog.close()
        api.alert('not implemented.')

    async def run(self, ctx):
        self.dialog = await api.showDialog(type='launchpad', data= [
                {'name': 'Train with data from the web', 'description': 'Use training data stored as zip provided with an url.', 'callback': self.train_with_url, 'img': 'https://img.icons8.com/color/96/000000/download-2.png'},
                {'name': 'Train with data from the engine', 'description': 'Use training data stored on a (local or remote) plugin engine.', 'callback': self.train_with_folder, 'img': 'https://img.icons8.com/color/96/000000/opened-folder.png'},
                {'name': 'Predict', 'description': 'Perform denoising on new images.', 'callback': self.predict_folder, 'img': 'https://img.icons8.com/color/96/000000/double-right.png'},
                {'name': 'Documentation', 'description': 'Show documentation.', 'callback': self.show_docs, 'img': 'https://img.icons8.com/color/96/000000/help.png'},
            ]
        )

api.export(ImJoyPlugin())
</script>
